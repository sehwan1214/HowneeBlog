{
  
    
        "post0": {
            "title": "Title",
            "content": "Start Machine Learning with Scikit-Learn . Making Machine Learning model - Predicting the varieties of irises. | Learn Scikit-Learn based framework | Model_Selection | 1. Making Machine Learning model - Predicting the varieties of irises. . Create a data set -&gt; load_iris() ML algorithm -&gt; DecisionTreeClassifier Separate data into train data and test data -&gt; train_test_split() . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split . import pandas as pd # Load iris data iris = load_iris() # iris.data has data with only features as a numpy iris_data = iris.data # iris.target has label(decided value)data as a numpy iris_label = iris.target print(&#39;iris target &#39;s value:&#39;, iris_label) print(&#39;iris target &#39;s name:&#39;, iris.target_names) # Convert to DataFrame iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names) iris_df[&#39;label&#39;] = iris.target iris_df.head(3) . iris target&#39;s value: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] iris target&#39;s name: [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state = 11) . dtc = DecisionTreeClassifier(random_state = 11) dtc.fit(x_train, y_train) pred = dtc.predict(x_test) from sklearn.metrics import accuracy_score print(&quot;accuracy_score: %.4f&quot; %accuracy_score(y_test, pred)) . accuracy_score: 0.9333 . 2. Learn Scikit-Learn based framework . Understanding of Estimator, fit( ), predict( ) | The main process of building a Machine Learning model | 1. Understanding of Estimator, fit( ), predict( ) . Model Learning -&gt; fit() Prediction of the learned model -&gt; predict() Classification algorithm -&gt; Classifer Regression algorithm -&gt; Regressor Classifier + Regressor = Estimator(A class that implements all algorithms of supervised learning) Estimater implements fit() and predict() internally . 2. The main process of building a Machine Learning model . 1. Feature processing(processing, changing, extracting) 2. ML algorithm train/predict 3. Model evaluation . 3. Model_Selection . Seperate train data and test data, cross-validation division and evaluation, tune Estimator&#39;s hyper parameter . train_test_split( ) | cross-validation | GridSearchCV | Data Preprocessing | Feature Scaling and Normalization | 1. train_test_split( ) . ※Core parameters: test_size, random_state test_size -&gt; test_data size / whole data size random_state -&gt; Create the same training/test data set every time . train_data = iris.data train_label = iris.target dtc.fit(train_data, train_label) pred = dtc.predict(train_data) print(&quot;accuracy_score:&quot;, accuracy_score(pred, train_label)) . accuracy_score: 1.0 . x_train, x_test, y_train, y_test = train_test_split(train_data, train_label, test_size = 0.3, random_state = 121) dtc.fit(x_train, y_train) pred = dtc.predict(x_test) print(&quot;accuracy_score: %.4f&quot; %accuracy_score(y_test,pred)) . accuracy_score: 0.9556 . 2. cross-validation . Used for improve overfitting overfitting: When the model is excessively optimized only for train data and actual prediction is performed with other data, the prediction performance is excessively low. Train data -&gt; Train data + Validation data . KFOLD cross-validation . Create k data fold sets and repeatedly perform training and verification evaluations on each fold set as many as k times. data set(k=5) Train, Train, Train, Train, Valid -&gt; verification evaluation1(eval1) Train, Train, Train, Valid, Train -&gt; verification evaluation2(eval2) Train, Train, Valid, Train, Train -&gt; verification evaluation3(eval3) Train, Valid, Train, Train, Train -&gt; verification evaluation4(eval4) Valid, Train, Train, Train, Train -&gt; verification evaluation5(eval5) Final evaluation = mean(eval[1,2,3,4,5]) kfold.split() -&gt; split learning data into learning data and validation data return value: index . | . from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold import numpy as np iris = load_iris() features = iris.data label = iris.target dtc = DecisionTreeClassifier(random_state = 156) kfold = KFold(n_splits = 5) print(&quot;iris data&#39;s size:&quot;, features.shape[0]) . iris data&#39;s size: 150 . n_iter = 0 cv_accuracy = [] for train_index, test_index in kfold.split(features): x_train, x_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] dtc.fit(x_train, y_train) pred = dtc.predict(x_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred),4) cv_accuracy.append(accuracy) train_size = x_train.shape[0] test_size = x_test.shape[0] print(&quot; n#{0} validation accuracy:{1}, train_data size:{2}, test_data size:{3}&quot;.format(n_iter, accuracy, train_size, test_size)) print(&quot; n#{0} validation_data index:{1}&quot;.format(n_iter, test_index)) print(&quot; n## Final evaluation:&quot;, np.mean(cv_accuracy)) . #1 validation accuracy:1.0, train_data size:120, test_data size:30 #1 validation_data index:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] #2 validation accuracy:0.9667, train_data size:120, test_data size:30 #2 validation_data index:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] #3 validation accuracy:0.8667, train_data size:120, test_data size:30 #3 validation_data index:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] #4 validation accuracy:0.9333, train_data size:120, test_data size:30 #4 validation_data index:[ 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119] #5 validation accuracy:0.7333, train_data size:120, test_data size:30 #5 validation_data index:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## Final evaluation: 0.9 . Stratified KFold The k-fold method for label datasets with unbalanced distribution Frequently used in Classification problems Not supported in Regression Split (train_data) into (train_data) and (validation_data) by (label_data)&#39;s distribution -&gt; StratifiedKFold.split() -&gt; parameters: feature_data, label_data KFold.split() -&gt; parameters: feature_data . | . import pandas as pd iris = load_iris() iris_df = pd.DataFrame(data = iris.data, columns = iris.feature_names) iris_df[&#39;label&#39;] = iris.target iris_df[&#39;label&#39;].value_counts() . 2 50 1 50 0 50 Name: label, dtype: int64 . kfold = KFold(n_splits = 3) n_iter = 0 for train_index, test_index in kfold.split(iris_df): n_iter += 1 label_train = iris_df[&#39;label&#39;].iloc[train_index] label_test = iris_df[&#39;label&#39;].iloc[test_index] print(&quot;## cross-validation: {0}&quot;.format(n_iter)) print(&quot;train_label_data&#39;s distribution: n&quot;, label_train.value_counts()) print(&quot;test_label_data&#39;s distribution: n&quot;, label_test.value_counts()) print() . ## cross-validation: 1 train_label_data&#39;s distribution: 2 50 1 50 Name: label, dtype: int64 test_label_data&#39;s distribution: 0 50 Name: label, dtype: int64 ## cross-validation: 2 train_label_data&#39;s distribution: 2 50 0 50 Name: label, dtype: int64 test_label_data&#39;s distribution: 1 50 Name: label, dtype: int64 ## cross-validation: 3 train_label_data&#39;s distribution: 1 50 0 50 Name: label, dtype: int64 test_label_data&#39;s distribution: 2 50 Name: label, dtype: int64 . from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold(n_splits = 3) n_iter = 0 for train_index, test_index in skf.split(iris_df, iris_df[&#39;label&#39;]): n_iter += 1 label_train = iris_df[&#39;label&#39;].iloc[train_index] label_test = iris_df[&#39;label&#39;].iloc[test_index] print(&quot;## cross-validation: {0}&quot;.format(n_iter)) print(&quot;train_label_data&#39;s distribution: n&quot;, label_train.value_counts()) print(&quot;test_label_data&#39;s distribution: n&quot;, label_test.value_counts()) print() . ## cross-validation: 1 train_label_data&#39;s distribution: 2 34 1 33 0 33 Name: label, dtype: int64 test_label_data&#39;s distribution: 1 17 0 17 2 16 Name: label, dtype: int64 ## cross-validation: 2 train_label_data&#39;s distribution: 1 34 2 33 0 33 Name: label, dtype: int64 test_label_data&#39;s distribution: 2 17 0 17 1 16 Name: label, dtype: int64 ## cross-validation: 3 train_label_data&#39;s distribution: 0 34 2 33 1 33 Name: label, dtype: int64 test_label_data&#39;s distribution: 2 17 1 17 0 16 Name: label, dtype: int64 . n_iter = 0 cv_accuracy = [] for train_index, test_index in skf.split(features, label): x_train, x_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] dtc.fit(x_train, y_train) pred = dtc.predict(x_test) n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred),4) cv_accuracy.append(accuracy) train_size = x_train.shape[0] test_size = x_test.shape[0] print(&quot; n#{0} validation accuracy:{1}, train_data size:{2}, test_data size:{3}&quot;.format(n_iter, accuracy, train_size, test_size)) print(&quot; n#{0} validation_data index:{1}&quot;.format(n_iter, test_index)) print(&quot; n## Final evaluation:&quot;, np.mean(cv_accuracy)) . #1 validation accuracy:0.98, train_data size:100, test_data size:50 #1 validation_data index:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115] #2 validation accuracy:0.94, train_data size:100, test_data size:50 #2 validation_data index:[ 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132] #3 validation accuracy:0.98, train_data size:100, test_data size:50 #3 validation_data index:[ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## Final evaluation: 0.9666666666666667 . cross_val_score( ) cross-validation API Use Stratified KFold: Classfier KFold: Regressor cross_val_score(estinamter, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=&#39;2*n_jobs&#39;) ※Core parameters: estimator, X, y, scoring, cv estimatior -&gt; Classifier or Regressor X -&gt; Feature_data y -&gt; Label_data scoring -&gt; Evaluation metrics cv -&gt; Number of cross-validation folds return -&gt; Evaluation value with metrics . | . from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score from sklearn.datasets import load_iris iris_data = load_iris() dtc = DecisionTreeClassifier(random_state = 156) data = iris.data label = iris.target scores = cross_val_score(dtc, data, label, scoring = &#39;accuracy&#39;, cv=3) print(&quot;cross-validation score:&quot;, np.round(scores,4)) print(&quot;mean(cross-validation score):&quot;, np.round(np.mean(scores),4)) . cross-validation score: [0.98 0.94 0.98] mean(cross-validation score): 0.9667 . 3. GridSearchCV . API that can derive optimal parameters ※Core parameters: estimator, param_grid, scoring, cv, refit estimator -&gt; Classifier, Regressosr, Pipeline param_grid -&gt; dictionary(key + list) key: parameter name(str), value: parameter value(list) scoring -&gt; Evaluation metrics cv -&gt; Number of cross-validation folds refit -&gt; If true, find optimal hyperparameter, re-train the entered estimator object with the corresponding hyperparameters . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV iris_data = load_iris() x_train, x_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.2 ,random_state = 121) dtc = DecisionTreeClassifier() parameters = {&#39;max_depth&#39;: [1,2,3], &#39;min_samples_split&#39;: [2,3]} . import pandas as pd grid_dtc = GridSearchCV(dtc, param_grid = parameters, cv = 3, refit = True) grid_dtc.fit(x_train, y_train) scores_df = pd.DataFrame(grid_dtc.cv_results_) scores_df[[&#39;params&#39;, &#39;mean_test_score&#39;, &#39;rank_test_score&#39;, &#39;split0_test_score&#39;, &#39;split1_test_score&#39;, &#39;split2_test_score&#39;]] . params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score . 0 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 2} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 1 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 3} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 2 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 2} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 3 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 3} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 4 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . 5 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 3} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . bestparams, bestscore, bestestimator best_params_ -&gt; Hyperparameter values representing the highest performance best_score_ -&gt; Evaluation result representing the highest performance best_estimator_ -&gt; Learn estimator as a hyperparameter that represents optimal performance and save it as a best_estimator_ . | . estimator = grid_dtc.best_estimator_ pred = estimator.predict(x_test) print(&quot;Accuracy: %.4f&quot; %accuracy_score(y_test, pred)) print(&quot;hyperparameters:&quot;, grid_dtc.best_params_) . Accuracy: 0.9667 hyperparameters: {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} . 4. Data Preprocessing . Process Null(NaN) value -&gt; NaN is not allowed Convert string to number -&gt; In Scikit-Learn&#39;s ML algorithm, string values are not allowed as input values . Label encoding | One-Hot encoding | Feature Scaling and Normalizer | Precautions for scaling conversion of training data and test data | 1. Label encoding . Convert category features to code-type numerical values ex) category features: a,b,c,d,e After label encoding -&gt; a:1, b:2, c:3, d:4, e:5 In a particular ML algorithm, the predictive performance is poor because the characteristics of large and small work for numerical values So, don&#39;t apply label encoding to ML algorithm such as linear regression Tree-based algorithm -&gt; Does not reflect the characteristics of large and small numbers: No problem for label encoding Solution: One-Hot encoding . from sklearn.preprocessing import LabelEncoder le = LabelEncoder() items = [&#39;TV&#39;, &#39;refrigerator&#39;, &#39;oven&#39;, &#39;computer&#39;, &#39;electric fan&#39;, &#39;electric fan&#39;, &#39;mixer&#39;, &#39;mixer&#39;] le.fit(items) labels = le.transform(items) print(&quot;Before label encoding:&quot;, items) print(&quot;After label encoding:&quot;, labels) . Before label encoding: [&#39;TV&#39;, &#39;refrigerator&#39;, &#39;oven&#39;, &#39;computer&#39;, &#39;electric fan&#39;, &#39;electric fan&#39;, &#39;mixer&#39;, &#39;mixer&#39;] After label encoding: [0 5 4 1 2 2 3 3] . print(&#39;encoding class:&#39;, le.classes_) print(&quot; [ 0 1 2 3 4 5 ]&quot;) . encoding class: [&#39;TV&#39; &#39;computer&#39; &#39;electric fan&#39; &#39;mixer&#39; &#39;oven&#39; &#39;refrigerator&#39;] [ 0 1 2 3 4 5 ] . print(&#39;decoding value:&#39;, le.inverse_transform([0,5,4,1,2,2,3,3])) . decoding value: [&#39;TV&#39; &#39;refrigerator&#39; &#39;oven&#39; &#39;computer&#39; &#39;electric fan&#39; &#39;electric fan&#39; &#39;mixer&#39; &#39;mixer&#39;] . 2. One-Hot encoding . Depending on the type of feature value, a new feature is added to display 1 only in columns corresponding to unique values and 0 in the remaining columns Before applying One-Hot encoding, 1. Convert all string values to numerical values 2. 2D data is needed for input values . from sklearn.preprocessing import OneHotEncoder import numpy as np items = [&#39;TV&#39;, &#39;refrigerator&#39;, &#39;oven&#39;, &#39;computer&#39;, &#39;electric fan&#39;, &#39;electric fan&#39;, &#39;mixer&#39;, &#39;mixer&#39;] #1. Convert all string values to numerical values le = LabelEncoder() le.fit(items) labels = le.transform(items) #2. 2D data is needed for input values labels = labels.reshape(-1,1) #3. Apply One-Hot encoding ohe = OneHotEncoder() ohe.fit(labels) ohe_labels = ohe.transform(labels) print(&quot;One-Hot encoding data: n&quot;, ohe_labels.toarray()) print() print(&quot;Dimension of One-Hot encoding data: n&quot;, ohe_labels.shape ) . One-Hot encoding data: [[1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 0. 1. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.]] Dimension of One-Hot encoding data: (8, 6) . ohe_labels . &lt;8x6 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 8 stored elements in Compressed Sparse Row format&gt; . get_dummies( ) API, which supports One-Hot encoding more easily . | . import pandas as pd df = pd.DataFrame({&#39;item&#39; : [&#39;TV&#39;, &#39;refrigerator&#39;, &#39;oven&#39;, &#39;computer&#39;, &#39;electric fan&#39;, &#39;electric fan&#39;, &#39;mixer&#39;, &#39;mixer&#39;]}) pd.get_dummies(df) . item_TV item_computer item_electric fan item_mixer item_oven item_refrigerator . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 1 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 1 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 1 | 0 | 0 | 0 | . 5 0 | 0 | 1 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 1 | 0 | 0 | . 7 0 | 0 | 0 | 1 | 0 | 0 | . 3. Feature Scaling and Normalizer . Feature Scaling -&gt; StandardScaler(Standardization), MinMaxScaler(Normalization) Normalizer -&gt; Normalization of the concept of linear algebra(Vector normalization) . Standardization, Normalization, Normalizer . &lt; Standardization &gt; Convert each data feature to a value with a Gaussian normal distribution with a mean of 0 and a variance of 1 x_new = (x - mean(x)) / std(x) &lt; Normalization &gt; Transform the sizes to unify the sizes of different features(min:0, max:1) x_new = (x - min(x)) / (max(x) - min(x)) &lt; Normalizer &gt; Transform to size individual vectors x_new = x / sqrt(x^2 + y^2 + z^2) . | . StandardScaler Convert each data feature with a mean of 0 and a variance of 1 ML algorithm(SVM, Linear Regression, Logistic Regression) is implemented assuming that data has Gaussian distribution If standardization is applied: DataFrame -&gt; Numpy.ndarray The fit() and transform() of StandardScaler can only be done with 2D or higher data . | . from sklearn.datasets import load_iris import pandas as pd iris = load_iris() iris_data = iris.data iris_df = pd.DataFrame(iris.data, columns = iris.feature_names) print(&quot;mean_features&quot;) print(iris_df.mean()) print() print(&quot;variance_features&quot;) print(iris_df.var()) . mean_features sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 variance_features sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 . from sklearn.preprocessing import StandardScaler sc = StandardScaler() sc.fit(iris_df) iris_scaled = sc.transform(iris_df) iris_df_scaled = pd.DataFrame(iris_scaled, columns = iris.feature_names) print(&quot;mean_features&quot;) print(iris_df_scaled.mean()) print() print(&quot;variance_features&quot;) print(iris_df_scaled.var()) . mean_features sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 variance_features sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 . MinMaxScaler Transform the size of features(min:0, max:1) If there&#39;s a negative value, (min:-1, max:1) If Normalization is applied: DataFrame -&gt; Numpy.ndarray The fit() and transform() of MinMaxScaler can only be done with 2D or higher data . | . from sklearn.preprocessing import MinMaxScaler msc = MinMaxScaler() msc.fit(iris_df) iris_scaled = msc.transform(iris_df) iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names) print(&quot;min_features&quot;) print(iris_df_scaled.min()) print() print(&quot;max_features&quot;) print(iris_df_scaled.max()) . min_features sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 max_features sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 . 4. Precautions for scaling conversion of training data and test data . fit() -&gt; Setting standard information for data conversion transform() -&gt; Convert data with set information * Problems ex) sc.fit(x_train) sc.transform(x_test) -&gt; sc is applied to x_train(standard information for data conversion: train_data) test_data&#39;s scaling standard information is different from train_data&#39;s So, transform(x_test) fail to derive the correct prediction results . Problems when scaling train_array&#39;s [1, 2, 3, 4, 5] -&gt; [0.1, 0.2, 0.3, 0.4, 0.5] test_array&#39;s [1, 2, 3, 4, 5] -&gt; [0.2, 0.4, 0.6, 0.8, 1] Scaled result is different between train_data and test_data . | . from sklearn.preprocessing import MinMaxScaler import numpy as np train_array = np.arange(0,11).reshape(-1,1) test_array = np.arange(0,6).reshape(-1,1) . msc = MinMaxScaler() msc.fit(train_array) train_scaled = msc.transform(train_array) print(&quot;original train_data:&quot;, train_array.reshape(-1)) print(&quot;scaled train_data:&quot;, np.round(train_scaled.reshape(-1),2)) . original train_data: [ 0 1 2 3 4 5 6 7 8 9 10] scaled train_data: [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ] . msc.fit(test_array) test_scaled = msc.transform(test_array) print(&quot;original test_data:&quot;, test_array.reshape(-1)) print(&quot;scaled test_data:&quot;, np.round(test_scaled.reshape(-1),2)) . original test_data: [0 1 2 3 4 5] scaled test_data: [0. 0.2 0.4 0.6 0.8 1. ] . Solutions for problems ML models are learned based on train data, test data must follow the scaling criteria of train data . | . msc = MinMaxScaler() msc.fit(train_array) train_scaled = msc.transform(train_array) print(&quot;original train_data:&quot;, train_array.reshape(-1)) print(&quot;scaled train_data:&quot;, np.round(train_scaled.reshape(-1),2)) print() test_scaled = msc.transform(test_array) print(&quot;original test_data:&quot;, test_array.reshape(-1)) print(&quot;scaled test_data:&quot;, np.round(test_scaled.reshape(-1),2)) . original train_data: [ 0 1 2 3 4 5 6 7 8 9 10] scaled train_data: [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ] original test_data: [0 1 2 3 4 5] scaled test_data: [0. 0.1 0.2 0.3 0.4 0.5] .",
            "url": "https://sehwan1214.github.io/HowneeBlog/2021/10/16/_10_17_Start_Machine_Learning_with_Scikit_Learn.html",
            "relUrl": "/2021/10/16/_10_17_Start_Machine_Learning_with_Scikit_Learn.html",
            "date": " • Oct 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Understanding of Machine Learning . Classification of Machine Learning | Numpy | Data Handling - Pandas | 1. Classification of Machine Learning . Supervised - Learning | . Classification | Regression | Recommendation System | Visual/Speech recognition | Text Analysis, NLP | Unsupervised - Learning | . Clustering | Dimension Reduction | Reinforcement Learning | 2. Numpy . Numpy, which means numerical Python, is a representative package that helps Python create linear algebra-based programs easily. Understanding Numpy is so important in python-based Machine Learning. . Overview of Numpy | Ndarray&#39;s data type | Create ndarray comfortably - arange, zeros, ones | To change the dimension and size of ndarray - reshape() | Choose ndarray&#39;s dataset - indexing | Sort of matrix - sort(), argsort() | Linear algebra operation - Inner product and transposed matrix | 1. Overview of Numpy . Numpy-based data type = ndarray | Shape -&gt; Form of a tuple(row,columns) | ndarray.ndim | . import numpy as np array1 = np.array([1,2,3]) print(&#39;array1 type: &#39;, type(array1)) print(&#39;array1.shape: &#39;, array1.shape) array2 = np.array([[1,2,3],[4,5,6]]) print(&#39;array2 type: &#39;, type(array2)) print(&#39;array2.shape: &#39;, array2.shape) array3 = np.array([[1,2,3]]) print(&#39;array3 type: &#39;, type(array3)) print(&#39;array3.shape: &#39;, array3.shape) print(&quot;array1: {0}-dimensional, array2: {1}-dimensional, array3: {2}-dimensional&quot;.format(array1.ndim, array2.ndim, array3.ndim)) . array1 type: &lt;class &#39;numpy.ndarray&#39;&gt; array1.shape: (3,) array2 type: &lt;class &#39;numpy.ndarray&#39;&gt; array2.shape: (2, 3) array3 type: &lt;class &#39;numpy.ndarray&#39;&gt; array3.shape: (1, 3) array1: 1-dimensional, array2: 2-dimensional, array3: 2-dimensional . 2. ndarray&#39;s data type . Only the same data type can be used in ndarray If other data types are mixed, type conversion is applied to data types with larger data sizes . ndarray.dtype() Type conversion is applied to data types with larger data sizes . | . list1 = [1, 2, 3] print(type(list1)) array1 = np.array(list1) print(type(array1)) print(array1, array1.dtype) list2 = [1, 2, &#39;test&#39;] array2 = np.array(list2) print(array2, array2.dtype) list3 = [1, 2, 3.0] array3 = np.array(list3) print(array3, array3.dtype) . &lt;class &#39;list&#39;&gt; &lt;class &#39;numpy.ndarray&#39;&gt; [1 2 3] int64 [&#39;1&#39; &#39;2&#39; &#39;test&#39;] &lt;U21 [1. 2. 3.] float64 . ndarray.astype() Change the type of data for saving memory . | . array_int = np.array([1,2,3]) array_float = array_int.astype(&#39;float64&#39;) print(array_float, array_float.dtype) array_int1 = array_float.astype(&#39;int32&#39;) print(array_int1, array_int1.dtype) array_float1 = np.array([1.1, 2.1, 3.1]) array_int2 = array_float1.astype(&#39;int32&#39;) print(array_int2, array_int2.dtype) . [1. 2. 3.] float64 [1 2 3] int32 [1 2 3] int32 . 3. Create ndarray comfortably - arange, zeros, ones . Used to create data primarily for testing or to initialize large amounts of data collectively. . arange, zeros, ones | . sequence_array = np.arange(10) print(sequence_array) print(sequence_array.dtype, sequence_array.shape) zero_array = np.zeros((3,2), dtype=&#39;int32&#39;) print(zero_array) print(zero_array.dtype, zero_array.shape) one_array = np.ones((3,2)) print(one_array) print(one_array.dtype, one_array.shape) . [0 1 2 3 4 5 6 7 8 9] int64 (10,) [[0 0] [0 0] [0 0]] int32 (3, 2) [[1. 1.] [1. 1.] [1. 1.]] float64 (3, 2) . 4. To change the dimension and size of ndarray - reshape() . If -1 is used as a factor, it&#39;s going to be a new type that&#39;s compatible with ndarray. reshape(-1,1) -&gt; ndarray must be converted into 2-dimensional in any form with 1 column no matter how many rows it has. In conclusion, converting to 1-dimensional arrangement. . reshape | . array1 = np.arange(10) print(&#39;array1: n&#39;, array1) array2 = array1.reshape(2,5) print(&#39;array2: n&#39;, array2) array3 = array1.reshape(5,2) print(&#39;array3: n&#39;, array3) . array1: [0 1 2 3 4 5 6 7 8 9] array2: [[0 1 2 3 4] [5 6 7 8 9]] array3: [[0 1] [2 3] [4 5] [6 7] [8 9]] . Use -1 factor in reshape | . array1 = np.arange(10) print(array1) array2 = array1.reshape(-1, 5) print(&#39;array2.shape:&#39;, array2.shape) array3 = array1.reshape(5, -1) print(&#39;array3.shape:&#39;, array3.shape) . [0 1 2 3 4 5 6 7 8 9] array2.shape: (2, 5) array3.shape: (5, 2) . reshape(-1, 1) | . array1 = np.arange(8) array3d = array1.reshape((2,2,2)) print(&#39;array3d: n&#39;, array3d.tolist()) # 3-dimensional -&gt; 2-dimensional array5 = array3d.reshape(-1,1) print(&#39;array5: n&#39;, array5.tolist()) print(&#39;array5.shape:&#39;, array5.shape) #1-dimensional -&gt; 2-dimensional array6 = array1.reshape(-1,1) print(&#39;array6: n&#39;, array6.tolist()) print(&#39;array6.shape:&#39;, array6.shape) . array3d: [[[0, 1], [2, 3]], [[4, 5], [6, 7]]] array5: [[0], [1], [2], [3], [4], [5], [6], [7]] array5.shape: (8, 1) array6: [[0], [1], [2], [3], [4], [5], [6], [7]] array6.shape: (8, 1) . 5. Choose ndarray&#39;s dataset - indexing . * Extract only specific data: Specify the desired index value, return the location data. * Slicing: Extracting ndarrays on a continuous index. * Fancy indexing: A certain set of indexes is designated in the form of a list or ndarray and returns the ndarray of the data in the corresponding location. * Boolean indexing: Returns the end of data at the index location corresponding to the true based on a set of true/false value indexes that correspond to a specific condition. . Extract only specific data | . array1 = np.arange(1,10) print(&#39;array1:&#39;, array1) value = array1[2] print(&#39;value:&#39;, value) print(type(value)) print(&#39;The first value from the back: {0}, The second value from the back: {1}&#39;.format(array1[-1],array1[-2])) . array1: [1 2 3 4 5 6 7 8 9] value: 3 &lt;class &#39;numpy.int64&#39;&gt; The first value from the back: 9, The second value from the back: 8 . array2d = array1.reshape(3,3) print(array2d) value = array2d[1,1] print(&#39;array2d[1,1]:&#39;,value) . [[1 2 3] [4 5 6] [7 8 9]] array2d[1,1]: 5 . Slicing | . array1 = np.arange(1,10) array3 = array1[0:3] print(array3) print(type(array3)) . [1 2 3] &lt;class &#39;numpy.ndarray&#39;&gt; . array4 = array1[:3] print(array4) array5 = array1[3:] print(array5) array6 = array1[:] print(array6) . [1 2 3] [4 5 6 7 8 9] [1 2 3 4 5 6 7 8 9] . print(&#39;array2d: n&#39;, array2d) print(&#39;array2d[0:2,0:2]: n&#39;,array2d[0:2,0:2]) print(&#39;array2d[1:3,0:3]: n&#39;,array2d[1:3,0:3]) print(&#39;array2d[:,:]: n&#39;,array2d[:,:]) . array2d: [[1 2 3] [4 5 6] [7 8 9]] array2d[0:2,0:2]: [[1 2] [4 5]] array2d[1:3,0:3]: [[4 5 6] [7 8 9]] array2d[:,:]: [[1 2 3] [4 5 6] [7 8 9]] . print(array2d[0]) print(array2d[1]) print(&#39;array2d[0].shape:&#39;, array2d[0].shape, &#39;array2d[1].shape:&#39;,array2d[1].shape) . [1 2 3] [4 5 6] array2d[0].shape: (3,) array2d[1].shape: (3,) . Fancy indexing | . array1d = np.arange(1,10) array2d = array1d.reshape(3,3) array3 = array2d[[0,1],2] print(&#39;array2d[[0,1],2] =&gt; &#39;, array3.tolist()) array4 = array2d[[0,1],0:2] print(&#39;array2d[[0,1],0:2] =&gt; &#39;, array4.tolist()) array5 = array2d[[0,1]] print(&#39;array2d[[0,1]] =&gt; &#39;, array5.tolist()) . array2d[[0,1],2] =&gt; [3, 6] array2d[[0,1],0:2] =&gt; [[1, 2], [4, 5]] array2d[[0,1]] =&gt; [[1, 2, 3], [4, 5, 6]] . Boolean indexing | . array1d = np.arange(1,10) print(&#39;array1d:&#39;, array1d) print(&#39;array1d&gt;5:&#39;, array1d&gt;5) array3 = array1d[array1d&gt;5] print(&#39;array1d&gt;5 boolean indexing values:&#39;, array3) . array1d: [1 2 3 4 5 6 7 8 9] array1d&gt;5: [False False False False False True True True True] array1d&gt;5 boolean indexing values: [6 7 8 9] . indexes1 = np.array([5,6,7,8]) indexes2 = [5,6,7,8] array4_1 = array1d[indexes1] array4_2 = array1d[indexes2] print(array4_1) print(array4_2) . [6 7 8 9] [6 7 8 9] . 6. Sort of matrix - sort(), argsort() . default = ascending sort ex) [3,1,9,5] -&gt; [1,3,5,9] Sort by raw or column using axis argsort() -&gt; Use when an original matrix is sorted and an index for an element of an existing original matrix is required. Very useful in Numpy . ndarray.sort() | . org_array = np.array([3,1,9,5]) print(&quot;original_array:&quot;, org_array) sort_array_1 = org_array.sort() print(&quot;original_array:&quot;, org_array) print(&quot;sort_array1:&quot;, sort_array_1) . original_array: [3 1 9 5] original_array: [1 3 5 9] sort_array1: None . np.sort() | . org_array = np.array([3,1,9,5]) print(&quot;original_array:&quot;, org_array) sort_array_2 = np.sort(org_array) print(&quot;original_array:&quot;, org_array) print(&quot;sort_array2:&quot;, sort_array_1) . original_array: [3 1 9 5] original_array: [3 1 9 5] sort_array2: None . Descending sort -&gt; np.sort[::-1] | . sort_array1_desc = np.sort(org_array)[::-1] print(&quot;Descending sort array:&quot;, sort_array1_desc) . Descending sort array: [9 5 3 1] . Sort the 2-dimensional or higher matrix | . array2d = np.array([[8,12],[7,1]]) print(&#39;array2d: n&#39;, array2d) sort_array2d_axis0 = np.sort(array2d, axis = 0) print(&#39;Sorted by raw(axis=0): n&#39;, sort_array2d_axis0) sort_array2d_axis1 = np.sort(array2d, axis = 1) print(&#39;Sorted by column(axis=1): n&#39;, sort_array2d_axis1) . array2d: [[ 8 12] [ 7 1]] Sorted by raw(axis=0): [[ 7 1] [ 8 12]] Sorted by column(axis=1): [[ 8 12] [ 1 7]] . argsort() | . org_array = np.array([3,1,9,5]) sort_indices = np.argsort(org_array) print(type(sort_indices)) print(org_array) print(sort_array_2) print(&#39;index for an element of an existing original matrix when it is ascending sorted:&#39;, sort_indices) . &lt;class &#39;numpy.ndarray&#39;&gt; [3 1 9 5] [1 3 5 9] index for an element of an existing original matrix when it is ascending sorted: [1 0 3 2] . org_array = np.array([3,1,9,5]) sort_indices_desc = np.argsort(org_array)[::-1] print(type(sort_indices)) print(org_array) print(sort_array1_desc) print(&#39;index for an element of an existing original matrix when it is descending sorted:&#39;, sort_indices_desc) . &lt;class &#39;numpy.ndarray&#39;&gt; [3 1 9 5] [9 5 3 1] index for an element of an existing original matrix when it is descending sorted: [2 3 0 1] . 7. Linear algebra operation - Inner product and transposed matrix . Inner product -&gt; np.dot() | . A = np.array([[1,2,3],[4,5,6]]) B = np.array([[7,8],[9,10],[11,12]]) print(&quot;Inner product result: n&quot;, np.dot(A,B)) . Inner product result: [[ 58 64] [139 154]] . Transposed matrix | . A = np.array([[1,2],[3,4]]) transpose_mat = np. transpose(A) print(&#39;Transposed matrix: n&#39;, transpose_mat) . Transposed matrix: [[1 3] [2 4]] . 3. Data Handling - Pandas . Pandas provides a variety of great features to efficiently process 2D data. The core object of Pandas is DataFrame Before understanding the DataFrame, it is important to understand the index and series Index -&gt; A key value that uniquely identifies individual data Series -&gt; A structure with only one column DataFrame -&gt; A structure with lots of columns url(Data for this chapter): &#39;https://www.kaggle.com/c/titanic/data&#39; . Pandas - fundamental API | Interchange between DataFrame, List, Dictionary and ndarray | Delete Data | Index | Data Selection and Filtering | Sorting, Aggregation, GroupBy | Process missing data | Data processing with apply lambda | 1. Pandas - fundamental API . read_csv() - Loading data with DataFrame | . import pandas as pd titanic_df = pd.read_csv(&#39;train.csv&#39;) . head() -&gt; Return some data(Default:5) | . titanic_df.head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . shape() -&gt; Return the size of rows and columns in a DataFrame | . print(&#39;DataFrame &#39;s size: &#39;, titanic_df.shape) . DataFrame&#39;s size: (891, 12) . info() -&gt; row data size, column data size, data type, Null size | . titanic_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . describe() -&gt; Not Null size, mean, std, min, max, (25%, 50%, 75% values) Only the distribution of numeric columns(int, float) is investigated, and object-type columns are excluded from the output. . | . titanic_df.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . value_counts() -&gt; Return the number of data values of a specified column value_counts can only be called on series objects ex) DataFrame[column].value_counts . | . titanic_df[&#39;Pclass&#39;].value_counts() . 3 491 1 216 2 184 Name: Pclass, dtype: int64 . sort_values() ※ by -&gt; Column for sorting ex) DataFrame.sort_values(by=[column1, columns2 ..]) ascending = True -&gt; ascending order False -&gt; descending order . | . titanic_df.sort_values(by=&#39;Pclass&#39;, ascending = False) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 511 512 | 0 | 3 | Webber, Mr. James | male | NaN | 0 | 0 | SOTON/OQ 3101316 | 8.0500 | NaN | S | . 500 501 | 0 | 3 | Calic, Mr. Petar | male | 17.0 | 0 | 0 | 315086 | 8.6625 | NaN | S | . 501 502 | 0 | 3 | Canavan, Miss. Mary | female | 21.0 | 0 | 0 | 364846 | 7.7500 | NaN | Q | . 502 503 | 0 | 3 | O&#39;Sullivan, Miss. Bridget Mary | female | NaN | 0 | 0 | 330909 | 7.6292 | NaN | Q | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 102 103 | 0 | 1 | White, Mr. Richard Frasar | male | 21.0 | 0 | 1 | 35281 | 77.2875 | D26 | S | . 710 711 | 1 | 1 | Mayne, Mlle. Berthe Antonine (&quot;Mrs de Villiers&quot;) | female | 24.0 | 0 | 0 | PC 17482 | 49.5042 | C90 | C | . 711 712 | 0 | 1 | Klaber, Mr. Herman | male | NaN | 0 | 0 | 113028 | 26.5500 | C124 | S | . 712 713 | 1 | 1 | Taylor, Mr. Elmer Zebley | male | 48.0 | 1 | 0 | 19996 | 52.0000 | C126 | S | . 445 446 | 1 | 1 | Dodge, Master. Washington | male | 4.0 | 0 | 2 | 33638 | 81.8583 | A34 | S | . 891 rows × 12 columns . 2. Transforming between DataFrame, List, Dictionary and ndarray . Many API of Scikit-Learn use the ndarray as an input factor So, transforming between DataFrame and ndarray occurs very frequently . Transforming ndarray, List to DataFrame Must designate column&#39;s name for the DataFrame . | . import numpy as np list1 = [1,2,3] array1 = np.array(list1) df_list1 = pd.DataFrame(list1, columns = [&#39;col1&#39;]) df_array1 = pd.DataFrame(array1, columns = [&#39;col1&#39;]) print(&quot;List -&gt; DataFrame: n&quot;, df_list1) print(&quot;ndarray -&gt; DataFrame: n&quot;, df_array1) . List -&gt; DataFrame: col1 0 1 1 2 2 3 ndarray -&gt; DataFrame: col1 0 1 1 2 2 3 . Transforming Dictionary to DataFrame Dictionary -&gt; DataFrame Key -&gt; columns value -&gt; data . | . dict = {&#39;col1&#39;:[1,11], &#39;col2&#39;:[2,22], &#39;col3&#39;:[3,33]} df_dict = pd.DataFrame(dict) print(&quot;Dictionary -&gt; DataFrame: n&quot;, df_dict) . Dictionary -&gt; DataFrame: col1 col2 col3 0 1 2 3 1 11 22 33 . Transforming DataFrame to ndarray Transforming into ndarray using values is very important! . | . array3 = df_dict.values print(&quot;DataFrame -&gt; ndarray: n&quot;, array3) . DataFrame -&gt; ndarray: [[ 1 2 3] [11 22 33]] . Transforming DataFrame to List, Dictionary * DataFrame -&gt; List DataFrame.values.tolist() * DataFrame -&gt; Dictionary DataFrame.to_dict(&#39;list&#39;) . | . list3 = df_dict.values.tolist() print(&quot;DataFrame -&gt; List: n&quot;, list3) dict3 = df_dict.to_dict(&#39;list&#39;) print(&quot;DataFrame -&gt; Dictionary: n&quot;, dict3) #If don&#39;t use &#39;list&#39; in to_dict dict_3_error = df_dict.to_dict() print(&quot;DataFrame -&gt; Dictionary(error mode): n&quot;, dict_3_error) . DataFrame -&gt; List: [[1, 2, 3], [11, 22, 33]] DataFrame -&gt; Dictionary: {&#39;col1&#39;: [1, 11], &#39;col2&#39;: [2, 22], &#39;col3&#39;: [3, 33]} DataFrame -&gt; Dictionary(error mode): {&#39;col1&#39;: {0: 1, 1: 11}, &#39;col2&#39;: {0: 2, 1: 22}, &#39;col3&#39;: {0: 3, 1: 33}} . 3. Delete Data . DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors=&#39;raise&#39;) ※Core parameters: labels, axis, inplace labels -&gt; column to delete axis = 0 -&gt; Drop the specific row : Use when delete outlier data 1 -&gt; Drop the specific column : Primarily use inplace = True -&gt; Drop on original DataFrame = False -&gt; Original DataFrame is maintained . titanic_df.head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . Drop by columns | . titanic_drop_df_column = titanic_df.drop(&#39;Age&#39;, axis = 1) titanic_drop_df_column.head(1) . PassengerId Survived Pclass Name Sex SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 1 | 0 | A/5 21171 | 7.25 | NaN | S | . Drop by rows | . titanic_drop_df_row = titanic_df.drop([0],axis = 0) titanic_drop_df_row.head(1) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 4. Index . An object that uniquely identifies a record of DataFrame, Series. Index objects cannot be changed . Extract index | . titanic_df = pd.read_csv(&#39;train.csv&#39;) indexes = titanic_df.index print(indexes) print(indexes.values[:10]) . RangeIndex(start=0, stop=891, step=1) [0 1 2 3 4 5 6 7 8 9] . reset_index() A new index is assigned as a continuous numeric type, and the existing index is added as a new column name, &#39;index&#39; If the index is not continuous &#39;int&#39; type data, it is mainly used to turn it back into continuous &#39;int&#39; type data. When reset_index() is applied to a Series, DataFrame is converted. ※ parameters: drop (drop = True) -&gt; Existing indexes are not added as new columns and the series remains the same. . | . titanic_reset_df = titanic_df.reset_index(inplace = False) titanic_reset_df.head(3) . index PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . print(&quot;### Before reset_index ###&quot;) value_counts = titanic_df[&#39;Pclass&#39;].value_counts() print(value_counts) print(&quot;type =&gt; &quot;, type(value_counts)) print() print(&quot;### After reset_index ###&quot;) new_value_counts = value_counts.reset_index(inplace = False) print(new_value_counts) print(&quot;type =&gt; &quot;, type(new_value_counts)) . ### Before reset_index ### 3 491 1 216 2 184 Name: Pclass, dtype: int64 type =&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; ### After reset_index ### index Pclass 0 3 491 1 1 216 2 2 184 type =&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; . print(&quot;### (drop = True) ###&quot;) new_value_counts2 = value_counts.reset_index(inplace = False, drop = True) print(new_value_counts2) print(&quot;type =&gt; &quot;, type(new_value_counts2)) . ### (drop = True) ### 0 491 1 216 2 184 Name: Pclass, dtype: int64 type =&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; . 5. Data Selection and Filtering . iloc[] -&gt; Location-based indexing Use integar type as row and column value Don&#39;t provide boolean indexing loc[] -&gt; Name-based indexing loc[row, column] row : DataFrame&#39;s index, column : column&#39;s name DataFrame[] -&gt; Column name, Index can go into []operator . Extract column data - Column name | . print(&quot;Single column extraction&quot;) print(titanic_df[&#39;Pclass&#39;].head(3)) print() print(&quot;Several columns extraction&quot;) print(titanic_df[[&#39;Pclass&#39;,&#39;Survived&#39;]].head(3)) #print(titanic_df[0])-&gt; If you just enter a number value, it&#39;s an error. . Single column extraction 0 3 1 1 2 3 Name: Pclass, dtype: int64 Several columns extraction Pclass Survived 0 3 0 1 1 1 2 3 1 . Extract row data - Slicing | . titanic_df[0:2] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . Extract data by boolean indexing | . titanic_df[&#39;Pclass&#39;]==3 . 0 True 1 False 2 True 3 False 4 True ... 886 False 887 False 888 True 889 False 890 True Name: Pclass, Length: 891, dtype: bool . titanic_df[titanic_df[&#39;Pclass&#39;]==3].head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.250 | NaN | S | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.925 | NaN | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.050 | NaN | S | . iloc | . data = {&#39;name&#39;:[&#39;Chulmin&#39;, &#39;Eunkyung&#39;, &#39;Jinwoong&#39;, &#39;Soobeom&#39;], &#39;Year&#39;: [2011, 2016, 2015, 2015], &#39;Gender&#39;: [&#39;Male&#39;, &#39;Female&#39;, &#39;Male&#39;, &#39;Male&#39;]} data_df = pd.DataFrame(data, index = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;]) data_df . name Year Gender . one Chulmin | 2011 | Male | . two Eunkyung | 2016 | Female | . three Jinwoong | 2015 | Male | . four Soobeom | 2015 | Male | . print(&quot;data_df.iloc[0,0]:{0}, data_df.iloc[1,1]:{1}, data_df.iloc[2,2]:{2}&quot;.format(data_df.iloc[0,0],data_df.iloc[1,1],data_df.iloc[2,2])) . data_df.iloc[0,0]:Chulmin, data_df.iloc[1,1]:2016, data_df.iloc[2,2]:Male . data_df_reset = data_df.reset_index(inplace = False) data_df_reset.index = data_df_reset.index +1 data_df_reset . index name Year Gender . 1 one | Chulmin | 2011 | Male | . 2 two | Eunkyung | 2016 | Female | . 3 three | Jinwoong | 2015 | Male | . 4 four | Soobeom | 2015 | Male | . data_df_reset.iloc[0,1] . &#39;Chulmin&#39; . data_df_reset.iloc[1:3, 1] . 2 Eunkyung 3 Jinwoong Name: name, dtype: object . loc | . data_df.loc[&#39;one&#39;,&#39;name&#39;] . &#39;Chulmin&#39; . data_df_reset.loc[1,&#39;name&#39;] . &#39;Chulmin&#39; . data_df_reset.loc[1:3, &#39;name&#39;] . 1 Chulmin 2 Eunkyung 3 Jinwoong Name: name, dtype: object . loc with boolean index | . titanic_df.loc[titanic_df[&#39;Age&#39;]&gt;60, [&#39;Name&#39;,&#39;Age&#39;]].head(3) . Name Age . 33 Wheadon, Mr. Edward H | 66.0 | . 54 Ostby, Mr. Engelhart Cornelius | 65.0 | . 96 Goldschmidt, Mr. George B | 71.0 | . 6. Sorting, Aggregation, GroupBy . sort_values( ) Sorting of DataFrame, Series ※Core parameters: by, ascending, inplace by -&gt; If enter a specific column, sort it with that column ascending = True -&gt; ascending order(Default) = False -&gt; descending order . | . titanic_sorted = titanic_df.sort_values(by=[&#39;Name&#39;]) titanic_sorted.head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 845 846 | 0 | 3 | Abbing, Mr. Anthony | male | 42.0 | 0 | 0 | C.A. 5547 | 7.55 | NaN | S | . 746 747 | 0 | 3 | Abbott, Mr. Rossmore Edward | male | 16.0 | 1 | 1 | C.A. 2673 | 20.25 | NaN | S | . 279 280 | 1 | 3 | Abbott, Mrs. Stanton (Rosa Hunt) | female | 35.0 | 1 | 1 | C.A. 2673 | 20.25 | NaN | S | . titanic_sorted = titanic_df.sort_values(by=[&#39;Pclass&#39;, &#39;Name&#39;], ascending = False) titanic_sorted.head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 868 869 | 0 | 3 | van Melkebeke, Mr. Philemon | male | NaN | 0 | 0 | 345777 | 9.5 | NaN | S | . 153 154 | 0 | 3 | van Billiard, Mr. Austin Blyler | male | 40.5 | 0 | 2 | A/5. 851 | 14.5 | NaN | S | . 282 283 | 0 | 3 | de Pelsmaeker, Mr. Alfons | male | 16.0 | 0 | 0 | 345778 | 9.5 | NaN | S | . Aggregation( ) min(), max(), sum(), count() -&gt; aggregation If apply the aggregation to the DataFrame, the aggregation is applied to all columns . | . titanic_df.count() . PassengerId 891 Survived 891 Pclass 891 Name 891 Sex 891 Age 714 SibSp 891 Parch 891 Ticket 891 Fare 891 Cabin 204 Embarked 889 dtype: int64 . titanic_df[[&#39;Age&#39;,&#39;Fare&#39;]].mean() . Age 29.699118 Fare 32.204208 dtype: float64 . groupby( ) When groupby is applied, return the DataFrame Groupby Object If the aggregation is applied to the result of applying the groupby to the DataFrame, the aggregation is applied to all columns except for the groupby target column . | . titanic_groupby = titanic_df.groupby(by = &#39;Pclass&#39;) print(type(titanic_groupby)) . &lt;class &#39;pandas.core.groupby.generic.DataFrameGroupBy&#39;&gt; . titanic_groupby = titanic_df.groupby(&#39;Pclass&#39;).count() titanic_groupby . PassengerId Survived Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . Pclass . 1 216 | 216 | 216 | 216 | 186 | 216 | 216 | 216 | 216 | 176 | 214 | . 2 184 | 184 | 184 | 184 | 173 | 184 | 184 | 184 | 184 | 16 | 184 | . 3 491 | 491 | 491 | 491 | 355 | 491 | 491 | 491 | 491 | 12 | 491 | . titanic_groupby = titanic_df.groupby(&#39;Pclass&#39;)[[&#39;PassengerId&#39;,&#39;Survived&#39;]].count() titanic_groupby . PassengerId Survived . Pclass . 1 216 | 216 | . 2 184 | 184 | . 3 491 | 491 | . titanic_df.groupby(&#39;Pclass&#39;)[&#39;Age&#39;].agg([max,min]) . max min . Pclass . 1 80.0 | 0.92 | . 2 70.0 | 0.67 | . 3 74.0 | 0.42 | . agg_format = {&#39;Age&#39;:&#39;max&#39;, &#39;SibSp&#39;:&#39;sum&#39;, &#39;Fare&#39;:&#39;mean&#39;} titanic_df.groupby(&#39;Pclass&#39;).agg(agg_format) . Age SibSp Fare . Pclass . 1 80.0 | 90 | 84.154687 | . 2 70.0 | 74 | 20.662183 | . 3 74.0 | 302 | 13.675550 | . 7. Process missing data . missing data = Null(NaN) isna() -&gt; Check if it&#39;s NaN or not fillna() -&gt; Replace NaN with another value . isna( ) | . titanic_df.isna().head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 False | False | False | False | False | False | False | False | False | False | True | False | . 1 False | False | False | False | False | False | False | False | False | False | False | False | . 2 False | False | False | False | False | False | False | False | False | False | True | False | . titanic_df.isna().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . fillna( ) | . titanic_df[&#39;Cabin&#39;] = titanic_df[&#39;Cabin&#39;].fillna(&#39;C000&#39;) titanic_df.head(3) . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | C000 | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | C000 | S | . titanic_df[&#39;Age&#39;] = titanic_df[&#39;Age&#39;].fillna(titanic_df[&#39;Age&#39;].mean()) titanic_df[&#39;Embarked&#39;] = titanic_df[&#39;Embarked&#39;].fillna(titanic_df[&#39;Embarked&#39;].fillna(&#39;S&#39;)) titanic_df.isna().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 0 Embarked 0 dtype: int64 . 8. Data processing with apply lambda . lambda ex) lambda x: x**2 x -&gt; input parameter x**2 -&gt; result value(return value) If use lots of parameters, use map function . | . def get_square(a): return a**2 print(&quot;general function&#39;s result:&quot;, get_square(3)) lambda_square = lambda x: x**2 print(&quot;lambda function&#39;s result:&quot;, lambda_square(3)) . general function&#39;s result: 9 lambda function&#39;s result: 9 . a = [1,2,3] squares = map(lambda x: x**2, a) list(squares) . [1, 4, 9] . titanic_df[&#39;Name_len&#39;] = titanic_df[&#39;Name&#39;].apply(lambda x: len(x)) titanic_df[[&#39;Name&#39;,&#39;Name_len&#39;]].head(3) . Name Name_len . 0 Braund, Mr. Owen Harris | 23 | . 1 Cumings, Mrs. John Bradley (Florence Briggs Th... | 51 | . 2 Heikkinen, Miss. Laina | 22 | . titanic_df[&#39;Child_Adult&#39;] = titanic_df[&#39;Age&#39;].apply(lambda x: &#39;Child&#39; if x&lt;=15 else &#39;Adult&#39;) titanic_df[[&#39;Age&#39;,&#39;Child_Adult&#39;]].head(5) . Age Child_Adult . 0 22.0 | Adult | . 1 38.0 | Adult | . 2 26.0 | Adult | . 3 35.0 | Adult | . 4 35.0 | Adult | . titanic_df[&#39;Age_cat&#39;] = titanic_df[&#39;Age&#39;].apply(lambda x: &#39;Child&#39; if x&lt;=15 else (&#39;Adult&#39; if x&lt;=60 else &#39;Elderly&#39;)) titanic_df[&#39;Age_cat&#39;].value_counts() . Adult 786 Child 83 Elderly 22 Name: Age_cat, dtype: int64 . def get_category(age): cat = &#39;&#39; if age&lt;=15: cat = &#39;Baby&#39; elif age&lt;=12: cat = &#39;Child&#39; elif age&lt;=18: cat = &#39;Teenager&#39; elif age&lt;=25: cat = &#39;Student&#39; elif age&lt;=35: cat = &#39;Young Adult&#39; elif age&lt;=60: cat = &#39;Adult&#39; else: cat = &#39;Elderly&#39; return cat titanic_df[&#39;Age_cat&#39;] = titanic_df[&#39;Age&#39;].apply(lambda x: get_category(x)) titanic_df[[&#39;Age&#39;,&#39;Age_cat&#39;]].head(5) . Age Age_cat . 0 22.0 | Student | . 1 38.0 | Adult | . 2 26.0 | Young Adult | . 3 35.0 | Young Adult | . 4 35.0 | Young Adult | .",
            "url": "https://sehwan1214.github.io/HowneeBlog/2021/10/16/_10_10_Understanding_of_ML.html",
            "relUrl": "/2021/10/16/_10_10_Understanding_of_ML.html",
            "date": " • Oct 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "제목!!",
            "content": "&#45936;&#51060;&#53552; &#51204;&#52376;&#47532; &#44592;&#48277; . 데이터 실수화(Data Vectorization) | 데이터 정제(Data Cleaning) | 데이터 통합(Data Integration) | 데이터 축소(Data Reduction) | 데이터 변환(Data Transformation) | 데이터 불균형(Data Imbalance) | 1. &#45936;&#51060;&#53552; &#49892;&#49688;&#54868;(Data Vectorization) . 범주형 자료의 실수화(Categorial data Vectorization) | . Skikit-learn의 DictVectorizer 함수 | CSR 표현식 (Compressed Sparse Row) | 텍스트 자료의 실수화(Text data Vectorization) | . 단어의 출현 횟수를 이용한 데이터 실수화 | TF-IDF 기법 | &#48276;&#51452;&#54805; &#51088;&#47308;&#51032; &#49892;&#49688;&#54868;(Categorial data Vectorization) . x = [{&#39;city&#39;:&#39;seoul&#39;, &#39;temp&#39;:10.0},{&#39;city&#39;:&#39;Dubai&#39;, &#39;temp&#39;:33.5},{&#39;city&#39;:&#39;LA&#39;, &#39;temp&#39;:20.0}] x . [{&#39;city&#39;: &#39;seoul&#39;, &#39;temp&#39;: 10.0}, {&#39;city&#39;: &#39;Dubai&#39;, &#39;temp&#39;: 33.5}, {&#39;city&#39;: &#39;LA&#39;, &#39;temp&#39;: 20.0}] . from sklearn.feature_extraction import DictVectorizer vec = DictVectorizer(sparse=False) vec.fit_transform(x) # vec.fit_transform(x) 자체가 하나의 type을 가진다 print(type(vec.fit_transform(x))) . array([[ 0. , 0. , 1. , 10. ], [ 1. , 0. , 0. , 33.5], [ 0. , 1. , 0. , 20. ]]) . # 희소행렬의 메모리 낭비를 해결하기 위한 방법 vec1 = DictVectorizer(sparse=True) # 메모리를 줄이기 위해 sparse=True -&gt; 압축되어 있는 형태 x1 = vec1.fit_transform(x) x1 . &lt;3x4 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 6 stored elements in Compressed Sparse Row format&gt; . &#53581;&#49828;&#53944; &#51088;&#47308;&#51032; &#49892;&#49688;&#54868;(Text data Vectorization) . text = {&#39;떴다 떴다 비행기 날아라 날아라&#39;, &#39;높이 높이 날아라 우리 비행기&#39;, &#39;내가 만든 비행기 날아라 날아라&#39;, &#39;멀리 멀리 날아라 우리 비행기&#39;} text . {&#39;내가 만든 비행기 날아라 날아라&#39;, &#39;높이 높이 날아라 우리 비행기&#39;, &#39;떴다 떴다 비행기 날아라 날아라&#39;, &#39;멀리 멀리 날아라 우리 비행기&#39;} . from sklearn.feature_extraction.text import CountVectorizer vec2 = CountVectorizer() # default는 sparse=True t = vec2.fit_transform(text).toarray() # sparse=True를 풀고 text를 수량화 배열 자료로 변환, toarray() -&gt; CSR 표현의 압축을 풀기 위해 사용 import pandas as pd t1 = pd.DataFrame(t,columns=vec2.get_feature_names()) t1 . 날아라 내가 높이 떴다 만든 멀리 비행기 우리 . 0 1 | 0 | 0 | 0 | 0 | 2 | 1 | 1 | . 1 2 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | . 2 1 | 0 | 2 | 0 | 0 | 0 | 1 | 1 | . 3 2 | 0 | 0 | 2 | 0 | 0 | 1 | 0 | . # 1. 가중치 재계산 2. 높은 빈도에 낮은 가중치, 낮은 빈도에 높은 가중치 from sklearn.feature_extraction.text import TfidfVectorizer tfid = TfidfVectorizer() x2 = tfid.fit_transform(text).toarray() # 높은 빈도는 낮은 가중치, 낮은 빈도는 높은 가중치 x3 = pd.DataFrame(x2,columns=tfid.get_feature_names()) x3 . 날아라 내가 높이 떴다 만든 멀리 비행기 우리 . 0 0.450735 | 0.000000 | 0.00000 | 0.86374 | 0.000000 | 0.00000 | 0.225368 | 0.000000 | . 1 0.569241 | 0.545415 | 0.00000 | 0.00000 | 0.545415 | 0.00000 | 0.284620 | 0.000000 | . 2 0.229589 | 0.000000 | 0.00000 | 0.00000 | 0.000000 | 0.87992 | 0.229589 | 0.346869 | . 3 0.229589 | 0.000000 | 0.87992 | 0.00000 | 0.000000 | 0.00000 | 0.229589 | 0.346869 | . 2. &#45936;&#51060;&#53552; &#51221;&#51228;(Data Cleaning) . 결측 데이터 채우기(Empty Values) | . 결측 데이터: np.nan, npNAN, None | 평균(mean), 중위수(median), 최빈수(most frequent value)로 대처하는 기법 | 사용가능함수 -&gt; sklearn의 Imputer() | &#44208;&#52769; &#45936;&#51060;&#53552; &#52292;&#50864;&#44592;(Empty Values) . import numpy as np x_miss = np.array([[1,2,3,None],[5,np.NAN,7,8],[None,10,11,12],[13,np.NAN,15,16]]) x_miss . array([[1, 2, 3, None], [5, nan, 7, 8], [None, 10, 11, 12], [13, nan, 15, 16]], dtype=object) . from sklearn.impute import SimpleImputer im1 = SimpleImputer(strategy=&#39;mean&#39;) im1.fit_transform(x_miss) # 열의 평균값으로 대체 im2 = SimpleImputer(strategy=&#39;median&#39;) im2.fit_transform(x_miss) # 열의 중간값으로 대체 im3 = SimpleImputer(strategy=&#39;most_frequent&#39;) im3.fit_transform(x_miss) # 열의 최빈값으로 대체 . array([[ 1., 2., 3., 12.], [ 5., 6., 7., 8.], [ 5., 10., 11., 12.], [13., 6., 15., 16.]]) . 3. &#45936;&#51060;&#53552; &#53685;&#54633;(Data Integration) . 데이터 통합 | . Pandas의 merge()함수 사용 | Pandas의 df.dtypes -&gt; 변수의 자료 타입 확인 | &#45936;&#51060;&#53552; &#53685;&#54633; . #df1 import pandas as pd df1 = pd.read_csv(&quot;train.csv&quot;, engine=&#39;python&#39;) print(df1.shape) type(df1) df1.head() . (1017209, 9) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . df2 = pd.read_csv(&quot;store.csv&quot;, engine=&#39;python&#39;) print(df2.shape) type(df2) df2.head() . (1115, 10) . Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 1 2 | a | a | 570.0 | 11.0 | 2007.0 | 1 | 13.0 | 2010.0 | Jan,Apr,Jul,Oct | . 2 3 | a | a | 14130.0 | 12.0 | 2006.0 | 1 | 14.0 | 2011.0 | Jan,Apr,Jul,Oct | . 3 4 | c | c | 620.0 | 9.0 | 2009.0 | 0 | NaN | NaN | NaN | . 4 5 | a | a | 29910.0 | 4.0 | 2015.0 | 0 | NaN | NaN | NaN | . df = pd.merge(df1,df2,on=&#39;Store&#39;) print(df.shape) df.head() . (1017209, 18) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 1 1 | 4 | 2015-07-30 | 5020 | 546 | 1 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 2 1 | 3 | 2015-07-29 | 4782 | 523 | 1 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 3 1 | 2 | 2015-07-28 | 5011 | 560 | 1 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . 4 1 | 1 | 2015-07-27 | 6102 | 612 | 1 | 1 | 0 | 1 | c | a | 1270.0 | 9.0 | 2008.0 | 0 | NaN | NaN | NaN | . df.dtypes . Store int64 DayOfWeek int64 Date object Sales int64 Customers int64 Open int64 Promo int64 StateHoliday object SchoolHoliday int64 StoreType object Assortment object CompetitionDistance float64 CompetitionOpenSinceMonth float64 CompetitionOpenSinceYear float64 Promo2 int64 Promo2SinceWeek float64 Promo2SinceYear float64 PromoInterval object dtype: object . print(len(df[&#39;Store&#39;].unique())) print(len(df[&#39;Date&#39;].unique())) # unique -&gt; 독립적인 개수 print(df[&#39;DayOfWeek&#39;].value_counts()) # value_counts() -&gt; 해당 value의 개수 . 1115 942 1023 942 666 942 675 942 163 942 674 942 ... 900 758 902 758 903 758 904 758 512 758 Name: Store, Length: 1115, dtype: int64 . 5. &#45936;&#51060;&#53552; &#48320;&#54872;(Data Transformation) . 데이터 변환 법 | . 표준화(Standardization) | 정규화(Normalization) | &#45936;&#51060;&#53552; &#48320;&#54872; &#48277; . x = (x - mean(x)) / sd(x) # sd(x) -&gt; 표준편차 # 정규화(Normalization) x = (x - min(x)) / (max(x) - min(x)) # 정규화가 표준화보다 유용하다. 단, 데이터 특성이 bell-shape이거나 이상치(noise가 큼, empty data가 많음)가 있을 경우에는 표준화가 유용하다. . NameError Traceback (most recent call last) &lt;ipython-input-34-651a4821c23e&gt; in &lt;module&gt;() 1 # 표준화(Standardization) 2 -&gt; 3 x = (x - mean(x)) / sd(x) # sd(x) -&gt; 표준편차 NameError: name &#39;x&#39; is not defined . 6. &#45936;&#51060;&#53552; &#48520;&#44512;&#54805;(Data Imbalance) . 과소표집(undersampling) . | 과대표집(oversampling) . | . SMOTE(Synthetic minority oversampling technique) | ADASYN(adaptive synthetic sampling method) | from collections import Counter from sklearn.datasets import make_classification from imblearn.over_sampling import SMOTE, ADASYN # n_feature 변경, n_informative와 n_redundant 추가 x, y = make_classification(n_classes=3, weights=[0.03,0.07,0.9], n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, n_samples=200, random_state=10) print(&#39;Original dataset shape %s &#39; %Counter(y) ) . Original dataset shape Counter({2: 180, 1: 14, 0: 6}) . import matplotlib.pyplot as plt plt.scatter(x[:,0], x[:,1], marker = &#39;o&#39;, c=y, s=100, edgecolor = &quot;k&quot;, linewidth=1) plt.xlabel(&quot;$x_1$&quot;) plt.ylabel(&quot;$x_2$&quot;) plt.show() . &#44284;&#45824;&#54364;&#51665;(oversampling) . sm = SMOTE(random_state=42) x_res, y_res = sm.fit_resample(x,y) print(&#39;Resampled dataset shape %s&#39; % Counter(y_res)) . Resampled dataset shape Counter({2: 180, 1: 180, 0: 180}) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . import matplotlib.pyplot as plt plt.scatter(x_res[:,0], x_res[:,1], marker=&#39;o&#39;, c=y_res, s=100, edgecolor=&quot;k&quot;, linewidth=1) plt.xlabel(&quot;$x_1$&quot;) plt.ylabel(&quot;$x_2$&quot;) plt.show() . ada = ADASYN(random_state=0) x_syn, y_syn = ada.fit_resample(x,y) print(&#39;Resampled dataset shape from ADASYN %s&#39; % Counter(y_syn)) . Resampled dataset shape from ADASYN Counter({2: 180, 1: 179, 0: 178}) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . import matplotlib.pyplot as plt plt.scatter(x_syn[:,0], x_syn[:,1], marker=&#39;o&#39;, c=y_syn, s=100, edgecolor=&quot;k&quot;, linewidth=1) plt.xlabel(&quot;$x_1$&quot;) plt.ylabel(&quot;$x_2$&quot;) plt.show() . &#44284;&#49548;&#54364;&#51665;(undersampling) . from imblearn.under_sampling import NearMiss # define the undersampling method undersample = NearMiss(version=3, n_neighbors_ver3=3) # transform the dataset x_under, y_under = undersample.fit_resample(x,y) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . import matplotlib.pyplot as plt plt.scatter(x_under[:,0], x_under[:,1], marker=&#39;o&#39;, c=y_under, s=100, edgecolor=&quot;k&quot;, linewidth=1) plt.xlabel(&quot;$x_1$&quot;) plt.ylabel(&quot;$x_2$&quot;) plt.show() .",
            "url": "https://sehwan1214.github.io/HowneeBlog/jupyter/machine%20learning/2021/10/16/_10_04_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/jupyter/machine%20learning/2021/10/16/_10_04_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Oct 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Machine Learning Posting",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sehwan1214.github.io/HowneeBlog/machine%20learning/2021/10/04/Markdown_test.html",
            "relUrl": "/machine%20learning/2021/10/04/Markdown_test.html",
            "date": " • Oct 4, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sehwan1214.github.io/HowneeBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sehwan1214.github.io/HowneeBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sehwan1214.github.io/HowneeBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sehwan1214.github.io/HowneeBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}